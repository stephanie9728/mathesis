# Vid-C.py  â€”  Scene Graph -> User-facing Explanation (optimized)

import os
import sys
import time
import json
from pathlib import Path


def load_scene_graph(video_root: Path):
    graph_path = video_root / "yolo_scene_graph.json"
    assert graph_path.exists(), f"ç¼ºå°‘ {graph_path}ï¼Œè¯·å…ˆè¿è¡Œ Vid-B.py"
    graph = json.loads(graph_path.read_text())
    return graph_path, graph


def build_llm_inputs(graph: dict, top_k_nodes: int = 12, top_k_edges: int = 30):
    """
    æŠŠ scene graph å‹ç¼©æˆå‡ è¡Œå­—ç¬¦ä¸²ï¼Œç»™ LLM å½“è¾“å…¥ã€‚
    """
    nodes = graph.get("nodes", [])
    edges = graph.get("edges", [])

    # åªå–å‰ top_k_nodes ä¸ªèŠ‚ç‚¹ï¼ŒæŒ‰å‡ºç°æ¬¡æ•°æ’åº
    nodes_sorted = sorted(nodes, key=lambda x: -x.get("count", 0))[:top_k_nodes]
    nodes_for_llm = [f'{n["label"]}Ã—{n.get("count", 1)}' for n in nodes_sorted]

    # åªå–å‰ top_k_edges æ¡è¾¹ï¼ŒæŒ‰ count æ’
    edges_sorted = sorted(edges, key=lambda x: -x.get("count", 0))[:top_k_edges]
    rel_for_llm = [
        f'{e["s"]} â€”{e["r"]}â†’ {e["o"]} (Ã—{e.get("count", 1)})'
        for e in edges_sorted
    ]

    return nodes_for_llm, rel_for_llm


def try_init_llm():
    """
    ä¼˜å…ˆä½¿ç”¨ä½ ä»“åº“é‡Œçš„ LLM å°è£… LLM.prompt.LLMPrompterï¼Œ
    æ¨¡å‹é»˜è®¤è®¾æˆ gpt-4o-miniï¼Œç”¨ä¸€ä¸ªå¾ˆè½»é‡çš„èŠ‚æµé€»è¾‘ã€‚
    """
    try:
        from LLM.prompt import LLMPrompter

        class ThrottledPrompter(LLMPrompter):
            def __init__(self, *args, rpm_cap=10, **kwargs):
                # rpm_capï¼šæ¯åˆ†é’Ÿæœ€å¤§è¯·æ±‚æ•°ï¼Œè°ƒå¤§å¯ä»¥æ›´å¿«
                super().__init__(*args, **kwargs)
                self.min_interval = 60.0 / max(1, rpm_cap)
                self._last = 0.0

            def query(self, *a, **k):
                # ç®€å•èŠ‚æµï¼Œé¿å…è§¦å‘é€Ÿç‡é™åˆ¶
                wait = self.min_interval - (time.time() - self._last)
                if wait > 0:
                    time.sleep(wait)
                out = super().query(*a, **k)
                self._last = time.time()
                return out

        api_key = os.getenv("OPENAI_API_KEY")
        assert api_key, "æœªæ£€æµ‹åˆ° OPENAI_API_KEY ç¯å¢ƒå˜é‡"

        llm = ThrottledPrompter(
            gpt_version="gpt-4o-mini",  # å°æ¨¡å‹ï¼šæ›´å¿«æ›´ä¾¿å®œ
            api_key=api_key,
        )
        print("âœ… ä½¿ç”¨ LLM.prompt (gpt-4o-mini) ç”Ÿæˆè§£é‡Š")
        return llm

    except Exception as e:
        print("[WARN] LLM.prompt ä¸å¯ç”¨ï¼Œæ”¹ç”¨æœ¬åœ°å…œåº•è§£é‡Šã€‚åŸå› :", e)
        return None


def build_prompts(nodes_for_llm, rel_for_llm):
    """
    ä¼˜åŒ–è¿‡çš„ promptï¼Œè´´è¿‘ä½ çš„å®éªŒåœºæ™¯ï¼š
      - æ¡Œé¢ã€å¨åŠ¡åœºæ™¯
      - å¼ºè°ƒäºº / å·¥å…· / å®¹å™¨ / é£Ÿç‰©
      - 2â€“3 æ¡ bulletï¼Œç¬¬ä¸€äººç§°ã€éæŠ€æœ¯è¡¨è¾¾
    """
    sys_prompt = (
        "You are the verbal module of a home-assistant robot. "
        "You are talking to a non-technical user while they watch the robot work at a table.\n"
        "\n"
        "You receive a coarse scene graph (objects and spatial relations) from the camera. "
        "Using ONLY this information, briefly explain what you currently see.\n"
        "\n"
        "Guidelines:\n"
        "  - Speak in FIRST PERSON as the robot (use 'I').\n"
        "  - Use simple, friendly language, no technical terms "
        "    (do NOT mention 'bounding boxes', 'scores', 'graph', or 'detections').\n"
        "  - Focus on: the person, tools (knife, fork, spoon), containers "
        "    (cup, bowl, mug, bottle), and food items (fruit, apple, banana, cereal, milk).\n"
        "  - If a person is NEAR or HOLDING a tool or container, make that the main point.\n"
        "  - If a tool is near a food item on a table (e.g., knife near apple), mention that "
        "    as what I seem to be preparing.\n"
        "  - Keep it SHORT: 2â€“3 bullet points maximum.\n"
        "  - Do NOT guess about success or failure of the task, and do NOT apologize. "
        "    Just describe what I see and what I seem to be doing.\n"
    )

    user_prompt = (
        "Here is the scene graph summary from my camera.\n\n"
        "Objects (top-k):\n"
        "  - " + "\n  - ".join(nodes_for_llm or ["(none)"]) + "\n\n"
        "Relationships (sampled edges):\n"
        "  - " + "\n  - ".join(rel_for_llm or ["(none)"]) + "\n\n"
        "Please respond with 2â€“3 bullet points in plain English."
    )

    return sys_prompt, user_prompt


def llm_explain_scene(llm, nodes_for_llm, rel_for_llm, save_dir: Path):
    sys_prompt, user_prompt = build_prompts(nodes_for_llm, rel_for_llm)
    prompt = {"system": sys_prompt, "user": user_prompt}

    # ä¸ºäº†é€Ÿåº¦ï¼Œmax_tokens ä¸è¦å¤ªå¤§
    text, _ = llm.query(
        prompt=prompt,
        sampling_params={
            "temperature": 0.2,
            "max_tokens": 160,
        },
        save=False,
        save_dir=str(save_dir),
    )
    return text.strip()


def fallback_explanation(nodes_for_llm, rel_for_llm):
    """
    æ²¡æœ‰ LLM æ—¶çš„å…œåº•è§£é‡Šï¼šä¸ç”¨ç½‘ç»œï¼Œä¿è¯æµç¨‹èƒ½è·‘å®Œã€‚
    """
    if not nodes_for_llm and not rel_for_llm:
        return (
            "â€¢ I canâ€™t confidently recognize what is on the table from this view.\n"
            "â€¢ I will keep observing and adjusting as I work."
        )

    lines = []
    if nodes_for_llm:
        lines.append("â€¢ I can see: " + ", ".join(nodes_for_llm) + ".")
    if rel_for_llm:
        # åªå±•ç¤ºå°‘é‡å…³ç³»ï¼Œé¿å…å¤ªå•°å—¦
        lines.append("â€¢ Some important spatial relations: " + "; ".join(rel_for_llm[:3]) + ".")
    else:
        lines.append("â€¢ I don't detect any strong spatial relations between objects yet.")
    return "\n".join(lines)


def main():
    # ====== 1) è§£æè§†é¢‘ç›®å½• ======
    if len(sys.argv) > 1:
        video_root = Path(sys.argv[1]).resolve()
    else:
        # é»˜è®¤ï¼šæ°´æœåˆ‡å‰² demo
        video_root = Path("./camera_demo_fruit").resolve()

    print(f"ğŸ“‚ Vid-C ä½¿ç”¨è§†é¢‘ç›®å½•: {video_root}")
    graph_path, graph = load_scene_graph(video_root)

    # ====== 2) å‡†å¤‡ LLM è¾“å…¥ ======
    nodes_for_llm, rel_for_llm = build_llm_inputs(graph)

    # ====== 3) åˆå§‹åŒ– LLMï¼ˆå¦‚æœå¯ç”¨ï¼‰ ======
    llm = try_init_llm()

    # ====== 4) ç”Ÿæˆè§£é‡Š ======
    if llm is not None:
        try:
            text = llm_explain_scene(llm, nodes_for_llm, rel_for_llm, save_dir=video_root.parent)
        except Exception as e:
            print("[WARN] è°ƒç”¨ LLM å¤±è´¥ï¼Œæ”¹ç”¨å…œåº•è§£é‡Šã€‚åŸå› :", e)
            text = fallback_explanation(nodes_for_llm, rel_for_llm)
    else:
        text = fallback_explanation(nodes_for_llm, rel_for_llm)

    # ====== 5) ä¿å­˜åˆ°æ–‡æœ¬æ–‡ä»¶ ======
    out_txt = video_root / "yolo_scene_explanation.txt"
    out_txt.write_text(text, encoding="utf-8")

    print("\n=== LLM (user-facing) explanation ===")
    print(text)
    print("\nSaved:", out_txt)


if __name__ == "__main__":
    main()
